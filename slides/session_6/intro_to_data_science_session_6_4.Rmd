---
title: "Introduction to Data Science"
subtitle: "Session 6.4"
author: "Robert Clements"
date: ""
output:
  xaringan::moon_reader:
    css: rc_css.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.align = "center", fig.asp=.5, message = FALSE, warning = FALSE)
```

### Session 6.4 Outline

- Statistical learning
  + Regression
  + *Modern Data Science with R - Chapter 7.5-7.6*
  + *Introduction to Statistical Learning - Chapter 2.1 & 3.1*
---
class: inverse, center, middle
# Statistical Learning
---
### Why statistical learning?

EDA and visualization are powerful tools, but they only work well with a limited number of variables - there is only so much information our eyes can process.

Also recall that humans are good at seeing patterns that aren't there, so we need tools that can extract patterns from random noise and *explain* the variation in our data. Note though that statistical and machine learning models often will make the same mistake as humans and identify spurious correlations.

We can use statistical learning to identify and explain relationships between variables, as well as for prediction.

---
### Some notation:

X: our set of predictors, explanatory variables, independent variables, or *features.* - these are the inputs to the model.

Y: our response, dependent variable or *target* - the thing we're interested in predicting or explaining.

Residuals: the difference between the predicted value (according to our regression model) and the actual value. residual = observed - predicted

Relationship between Y and X:

$$\Huge Y \sim f(X) + \epsilon$$

where 

$$\Huge \epsilon = \mbox{random noise}$$
---
### Why statistical learning (why estimate f(X))?

.center[Inference vs Prediction]

**Inference**: understanding the way our Y variable is influenced by changes in our X variables.

- What features are associated with the target?
- What is the relationship between each feature and our target?
- Are the relationships adequately summarized linearly, or is it a more complex relationship?


**Prediction**: predicting values of Y given new values of X. 
---
### Estimating f

**Parametric** 

$$\huge f(X) = \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \ldots + \beta_{p}X_{p}$$

--

And **non-parametric**

$$\huge f(X) = \mbox{something you can't parameterize}$$
---
### Simple Linear Regression

With simple linear regression, we assume a linear relationship between X and Y.

**Parametric** 

$$\huge f(X) = \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \ldots + \beta_{p}X_{p}$$

We would love to know all of the $\Huge \beta$ values, but we won't, so we estimate them:

$$\huge \hat{f}(X) = \hat{\beta}_{0} + \hat{\beta}_{1}X_{1} + \hat{\beta}_{2}X_{2} + \ldots + \hat{\beta}_{p}X_{p}$$
---
### Regression

We are interested in the relationship between *price* and *carat*, with *price* as our target.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
diamonds_n <- diamonds %>% sample_n(2000) 
diamonds_n %>% ggplot() +
  geom_point(aes(x = carat, y = price), alpha = .1) +
  theme_bw()
```
---
### Regression

We are interested in the relationship between *price* and *carat*, with *price* as our target.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
diamonds_n %>% ggplot(aes(x = carat, y = price)) +
  geom_point(alpha = .1) +
  geom_smooth(method='lm',formula=y~x) +
  theme_bw()
```
---
### Regression

We are interested in the relationship between *price* and *carat*, with *price* as our target.

Regression model:
$$\Huge \mbox{price} = -2256 + 7756 \times \mbox{carat}$$

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
diamonds_n %>% ggplot(aes(x = carat, y = price)) +
  geom_point(alpha = .1) +
  geom_smooth(method='lm',formula=y~x) +
  theme_bw()
```
---
### Regression

How did we estimate this function?

$$\Huge \mbox{price} = -2256 + 7756 \times \mbox{carat}$$

We minimized the sum of the residuals (squared). Recall 

residuals = observed value - predicted value

The observed values of price are given in the data. The predicted values of price come from our function.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
d_lm <- lm(price ~ carat, data = diamonds)
diamonds_n <- diamonds %>% mutate(predicted = predict(d_lm))
diamonds_n %>% ggplot(aes(x = carat, y = price)) +
  geom_point(alpha = .1) +
  geom_smooth(method='lm',formula=y~x) +
  geom_segment(aes(x=carat, xend=carat, y=price, yend=predicted), alpha = .2) +
  theme_bw()
```
---
### Regression in R

There is, of course, a mathematical way to estimate our regression model. I'll leave that to you to read up on. But, to do it in R we will use the `lm()` function (linear model). For the formula argument, the target is on the left and the features are on the right.

```{r}
(diam_lm <- lm(formula = price ~ carat, data = diamonds))
```
---
### Interpreting the coefficients

$$\Huge \mbox{price} = -2256 + 7756 \times \mbox{carat}$$

For every one unit increase in carat, the price increases by $7756.

If carat = 0, average price is -$2256

---
class: inverse, center, middle

# End of Session 6.4